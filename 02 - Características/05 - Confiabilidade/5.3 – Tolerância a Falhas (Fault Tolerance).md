## Resumo

A sub-característica **Tolerância a Falhas (Fault Tolerance)** da ISO-IEC 25010 refere-se à capacidade de um sistema continuar operando mesmo na presença de falhas de hardware ou software. Para avaliá-la, utilizam-se métricas como **MTBF** (Mean Time Between Failures), **MTTR** (Mean Time To Repair) e **Disponibilidade** (porcentagem de tempo em que o sistema está operacional) ([PubNub](https://www.pubnub.com/learn/glossary/fault-tolerance/?utm_source=chatgpt.com "What is Fault Tolerance? System Design & Engineering - PubNub")). Na prática, a observação envolve testes de injeção de falhas (por exemplo, rodar ferramentas de Chaos Engineering) e monitoramento de logs e métricas de desempenho. Como estudo de caso, o **Apache Cassandra** demonstra tolerância a falhas ao replicar dados de forma peer-to-peer entre múltiplos nós, permitindo que o cluster mantenha serviço mesmo com queda de nós ([Wikipedia](https://en.wikipedia.org/wiki/Apache_Cassandra?utm_source=chatgpt.com "Apache Cassandra"), [Neontri](https://neontri.com/blog/apache-cassandra-use-cases/?utm_source=chatgpt.com "5 Apache Cassandra Use Cases in Big Data - Neontri")). Para medir, injeta-se falhas em nós e monitora-se latência e taxa de erros, evidenciando pontos fortes (ausência de ponto único de falha, escalabilidade linear) e fracos (compromisso com consistência eventual) ([Ksolves](https://www.ksolves.com/blog/big-data/apache-cassandra/uncover-the-inner-workings-of-apache-cassandra-an-architectural-exploration?utm_source=chatgpt.com "Understanding Apache Cassandra Architecture - Ksolves")). Ferramentas úteis incluem **Gremlin**, **Chaos Toolkit**, **Hystrix**, **Nginx** e **HaProxy** para simular e mitigar falhas ([DEVTALENTS](https://devtalents.com/building-fault-tolerant-software/?utm_source=chatgpt.com "Developing fault-tolerant software: best practices, process & tips")).

---

## O que diz a característica?

A **Tolerância a Falhas** na ISO-IEC 25010 está definida como “o grau em que um sistema, produto ou componente é capaz de operar de forma correta na presença de falhas de hardware e/ou software” ([Perforce](https://www.perforce.com/blog/qac/what-is-iso-25010?utm_source=chatgpt.com "What Is ISO 25010? | Perforce Software")). Esse conceito complementa outras sub-características de **Confiabilidade**, como **Maturidade**, **Disponibilidade** e **Recuperabilidade**, focando especificamente na capacidade de **resistir** a falhas sem interromper o serviço entregue ao usuário ([Codacy Blog](https://blog.codacy.com/iso-25010-software-quality-model?utm_source=chatgpt.com "An Exploration of the ISO/IEC 25010 Software Quality Model")).

---

## Como observar e medir em um software?

### Métricas principais

1. **MTBF (Mean Time Between Failures):** média de tempo entre ocorrências de falha; valores maiores indicam melhor tolerância ([PubNub](https://www.pubnub.com/learn/glossary/fault-tolerance/?utm_source=chatgpt.com "What is Fault Tolerance? System Design & Engineering - PubNub")).
    
2. **MTTR (Mean Time To Repair):** tempo médio para recuperação após falha; valores menores refletem recuperação mais rápida e maior tolerância ([PubNub](https://www.pubnub.com/learn/glossary/fault-tolerance/?utm_source=chatgpt.com "What is Fault Tolerance? System Design & Engineering - PubNub")).
    
3. **Disponibilidade (Availability):** porcentagem de tempo em que o sistema está operacional, calculada como
    
    \text{Disponibilidade} = \frac{\text{Tempo Total} - \text{Tempo de Indisponibilidade}}{\text{Tempo Total}} \times 100\% \] :contentReference[oaicite:8]{index=8}.

### Métodos de observação

- **Injeção de falhas (Chaos Engineering):** usar ferramentas como Gremlin ou Chaos Toolkit para simular falhas de nó, latência de rede ou erros de disco, e monitorar o comportamento do sistema ([DEVTALENTS](https://devtalents.com/building-fault-tolerant-software/?utm_source=chatgpt.com "Developing fault-tolerant software: best practices, process & tips")).
    
- **Monitoramento de logs e métricas:** coletar métricas de latência, throughput e erros usando sistemas como Prometheus e Grafana, observando degradações durante falhas simuladas.
    
- **Testes de recuperação:** interromper nós em ambientes de homologação e medir o tempo até o serviço voltar ao estado estável, validando MTTR e disponibilidade.
    

---

## Estudo de caso: Apache Cassandra

### O que foi observado

O **Apache Cassandra** é um banco de dados NoSQL distribuído, projetado para **alta disponibilidade** e **tolerância a falhas** por meio de:

- **Arquitetura peer-to-peer:** não existe nó mestre; todos os nós são iguais, eliminando ponto único de falha ([Wikipedia](https://en.wikipedia.org/wiki/Apache_Cassandra?utm_source=chatgpt.com "Apache Cassandra")).
    
- **Replicação de dados:** dados são replicados em múltiplos nós de acordo com o **fator de replicação**, garantindo continuidade mesmo se um nó falhar ([Neontri](https://neontri.com/blog/apache-cassandra-use-cases/?utm_source=chatgpt.com "5 Apache Cassandra Use Cases in Big Data - Neontri")).
    
- **Detecção de falhas:** utiliza o **Phi Accrual Failure Detector** para identificar nós offline e redirecionar operações automaticamente ([Wikipedia](https://en.wikipedia.org/wiki/Apache_Cassandra?utm_source=chatgpt.com "Apache Cassandra")).
    

### Como seria medido

1. **Testes de caos:** derrubar um ou mais nós em laboratório, enquanto monitora latência e taxa de erro; espera-se que a maioria das operações continue sem falha perceptível aos clientes.
    
2. **Medição de MTBF/MTTR:** registrar logs de falha e recuperação; calcular média entre eventos e média de tempo de restabelecimento.
    
3. **Disponibilidade percusiva:** avaliar disponibilidade real em produção usando ferramentas de APM (Application Performance Monitoring).
    

### Pontos fortes

- **Sem ponto único de falha** graças à arquitetura distribuída ([Ksolves](https://www.ksolves.com/blog/big-data/apache-cassandra/uncover-the-inner-workings-of-apache-cassandra-an-architectural-exploration?utm_source=chatgpt.com "Understanding Apache Cassandra Architecture - Ksolves")).
    
- **Escalabilidade linear:** basta adicionar nós para aumentar capacidade e resiliência sem alterar configuração de clientes ([Ksolves](https://www.ksolves.com/blog/big-data/apache-cassandra/uncover-the-inner-workings-of-apache-cassandra-an-architectural-exploration?utm_source=chatgpt.com "Understanding Apache Cassandra Architecture - Ksolves")).
    

### Pontos fracos

- **Consistência eventual:** em troca da alta disponibilidade, pode ocorrer leitura de dados desatualizados imediatamente após escrita.
    
- **Complexidade operacional:** configuração de replicação, tunning de parâmetros de falha e reequilíbrio requerem experiência.
    

---

## Dicas de ferramentas para medir e melhorar

- **Gremlin:** plataforma de Chaos Engineering para injetar falhas em ambientes controlados.
    
- **Chaos Toolkit:** ferramenta open source para criar experimentos de caos sob demanda ([DEVTALENTS](https://devtalents.com/building-fault-tolerant-software/?utm_source=chatgpt.com "Developing fault-tolerant software: best practices, process & tips")).
    
- **Hystrix:** biblioteca de tolerância a falhas para aplicações Java, implementando circuit breakers.
    
- **Nginx** e **HaProxy:** proxies reversos que suportam health checks e failover automático de serviços ([DEVTALENTS](https://devtalents.com/building-fault-tolerant-software/?utm_source=chatgpt.com "Developing fault-tolerant software: best practices, process & tips")).
    
- **Prometheus + Grafana:** para coletar e visualizar métricas de MTBF, MTTR e disponibilidade em tempo real.
    
- **Simian Army (Netflix):** conjunto de ferramentas que inclui o Chaos Monkey para simular falhas em produção.
    

Com estas práticas e ferramentas, equipes de desenvolvimento e operações podem não apenas **medir**, mas também **elevar** o grau de Tolerância a Falhas de seus sistemas, alinhando-se às exigências de Confiabilidade da ISO-IEC 25010.