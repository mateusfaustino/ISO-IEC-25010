Here’s an overview of how **User Error Protection**—one of the sub-characteristics of **Usability** in ISO/IEC 25010—helps make software more forgiving and resilient, how you can spot and quantify it, an illustrative case study (Gmail’s “Forgotten Attachment Detector”), and some practical tool recommendations for measuring and improving it.

A quick summary:  
User Error Protection is defined as the _degree to which a system prevents users from making operation errors_ (e.g., by warning, confirming, or automatically correcting) ([ISO 25000 Portal](https://iso25000.com/index.php/en/iso-25000-standards/iso-25010?utm_source=chatgpt.com "ISO/IEC 25010")). You can observe it by tracking **error rates**, **help-request counts**, **recovery times**, and **prevented-error events** in usability tests ([Survicate](https://survicate.com/blog/ux-metrics/?utm_source=chatgpt.com "6 UX Metrics and KPIs to Measure User Experience in 2025"), [CMSWire.com](https://www.cmswire.com/digital-experience/usability-testing-7-metrics-to-assess-ease-of-use/?utm_source=chatgpt.com "Usability Testing: 7 Metrics to Assess Ease of Use - CMS Wire")). In our case study of Gmail’s “Forgotten Attachment Detector,” the feature scans message text for keywords like “attached” and prompts users when no file is attached—drastically reducing lost-attachment incidents, though at the cost of occasional false alarms ([Web Applications Stack Exchange](https://webapps.stackexchange.com/questions/46729/what-text-triggers-the-gmail-warning-about-forgetting-attachments?utm_source=chatgpt.com "What text triggers the Gmail warning about forgetting attachments?"), [WIRED](https://www.wired.com/2008/09/-forgotten-attachment-detector-stops-some-gmail-gaffes?utm_source=chatgpt.com "'Forgotten Attachment Detector' Stops (Some) Gmail Gaffes")). To measure and improve User Error Protection, you can employ usability-testing platforms (e.g., Maze, UserTesting), analytics tools (e.g., Hotjar event funnels, Google Analytics error events), and automated checks (e.g., Selenium-driven scripts, Axe-Core static analysis) to capture both quantitative metrics and qualitative feedback.

---

## 1. O que diz a característica?

**User Error Protection** é definida como o _grau em que um sistema previne usuários contra erros de operação_ ([ISO 25000 Portal](https://iso25000.com/index.php/en/iso-25000-standards/iso-25010?utm_source=chatgpt.com "ISO/IEC 25010")). Em outras palavras, avalia-se se o software dispõe de salvaguardas – como confirmações, validações em tempo real, desfazer (undo), dicas de recuperação ou correções automáticas – para impedir que o usuário realize uma ação equivocada ou sofra as consequências de um erro simples (por exemplo, enviar um e-mail sem o anexo prometido) ([ISO](https://www.iso.org/obp/ui/?utm_source=chatgpt.com "ISO/IEC 25010:2023(en), Systems and software engineering")).

---

## 2. Como observar e medir em um software?

Para quantificar User Error Protection, combine **métricas de performance** (quantitativas) com **feedback de usuários** (qualitativos):

1. **Taxa de erros do usuário (User Error Rate)**
    
    - Número de erros observados (por tarefa ou por sessão) dividido pelo total de tentativas de tarefas ([Survicate](https://survicate.com/blog/ux-metrics/?utm_source=chatgpt.com "6 UX Metrics and KPIs to Measure User Experience in 2025")).
        
2. **Número de solicitações de ajuda (Help Requests)**
    
    - Quantas vezes o usuário clicou em “Ajuda” ou chamou suporte interno em decorrência de incertezas ([CMSWire.com](https://www.cmswire.com/digital-experience/usability-testing-7-metrics-to-assess-ease-of-use/?utm_source=chatgpt.com "Usability Testing: 7 Metrics to Assess Ease of Use - CMS Wire")).
        
3. **Tempo de recuperação de erro (Error Recovery Time)**
    
    - Tempo médio entre a ocorrência de um erro e a retomada bem-sucedida da tarefa.
        
4. **Eventos de prevenção de erro**
    
    - Contagem de prompts ou confirmações apresentados (e quantos o usuário cancelou) – por exemplo, avisos “Você esqueceu de anexar um arquivo?” ([Web Applications Stack Exchange](https://webapps.stackexchange.com/questions/46729/what-text-triggers-the-gmail-warning-about-forgetting-attachments?utm_source=chatgpt.com "What text triggers the Gmail warning about forgetting attachments?")).
        
5. **Severidade dos erros**
    
    - Classificação do impacto (leve, moderado, crítico) quando o usuário de fato comete o erro ([usertesting.com](https://www.usertesting.com/blog/user-experience-metrics-to-know?utm_source=chatgpt.com "9 user experience (UX) metrics you should know - UserTesting")).
        

Em estudos de usabilidade, utilize protocolos padronizados (por exemplo, tarefas de teste moderadas) e registre cada erro, clique em ajuda e passo de recuperação. Ferramentas de gravação de sessão (Hotjar, FullStory) e plataformas de teste remoto (Maze, UserTesting) ajudam a coletar esses dados.

---

## 3. Estudo de caso: Gmail e o “Forgotten Attachment Detector”

### 3.1 O que foi observado

O Gmail, em sua seção “Labs” (agora integrado nativamente), monitora o texto da mensagem em busca de palavras-chave como “attached,” “find attached,” “I have attached,” etc., e se não há arquivos anexados, exibe um diálogo de confirmação antes de enviar ([Web Applications Stack Exchange](https://webapps.stackexchange.com/questions/46729/what-text-triggers-the-gmail-warning-about-forgetting-attachments?utm_source=chatgpt.com "What text triggers the Gmail warning about forgetting attachments?"), [WIRED](https://www.wired.com/2008/09/-forgotten-attachment-detector-stops-some-gmail-gaffes?utm_source=chatgpt.com "'Forgotten Attachment Detector' Stops (Some) Gmail Gaffes")).

### 3.2 Como seria medido

- **Prevented-Error Events**: número de diálogos exibidos por dia;
    
- **Taxa de confirmação**: proporção de usuários que cancelam o envio após o aviso;
    
- **Taxa de incidentes**: antes e depois da ativação, reduziram-se drasticamente os chamados de suporte relatando “e-mail sem anexo”;
    
- **Tempo em caixa de diálogo**: média de tempo que usuários passam lendo o alerta.
    

### 3.3 Pontos fortes e fracos

- **Pontos fortes**:
    
    - **Alta efetividade** na redução de e-mails sem anexos;
        
    - **Feedback imediato** impede perda de dados/informação;
        
    - **Implementação transparente** sem atrapalhar demais o fluxo.
        
- **Pontos fracos**:
    
    - **Falsos positivos** quando se menciona “attachment” em contexto não relacionado a arquivos;
        
    - **Sobrecarga de alertas** se o usuário cometer muitas menções sem anexar deliberadamente;
        
    - **Dependência de palavras-chave** pode falhar em cenários multilingues.
        

---

## 4. Dicas de ferramentas para medir essa subcategoria

### 4.1 Plataformas de testes de usabilidade

- **Maze**: permite tarefas moderadas com métricas automáticas de taxa de erro, UER e tempo de tarefa ([Maze](https://maze.co/blog/measure-usability-metrics/?utm_source=chatgpt.com "7 Key Usability Metrics to Unlock User Insights - Maze")).
    
- **UserTesting**: grava sessões reais de usuários, captura comportamentos de erro e feedback verbal.
    

### 4.2 Ferramentas analíticas e de gravação

- **Hotjar / FullStory**: grava cliques, reproduz fluxos de erro e exibe mapas de calor de onde ocorrem problemas;
    
- **Google Analytics**: defina eventos para “click_warning” e “error_recovery” e monitore dimensões de usuários e sessões.
    

### 4.3 Automação e testes contínuos

- **Selenium / Playwright**: scripts automatizados que simulam erros (por exemplo, submeter formulário sem campo obrigatório) e validam a presença de alertas;
    
- **Axe-Core**: para identificar padrões de validação e feedback de erros no DOM.
    

### 4.4 Ferramentas de survey e NPS

- **Qualaroo / Hotjar Surveys**: perguntar logo após erro recuperado “Você achou fácil corrigir a falha?” para qualificar severidade e satisfação.
    

---

**Conclusão:**  
Proteção contra erros do usuário é essencial para tornar software mais robusto e satisfatório. Medindo **error rates**, **help requests**, **prevented-error events** e combinando dados quantitativos com testes moderados, você obtém um retrato fiel de quão bem o seu sistema ampara usuários em momentos de incerteza. Ferramentas como Maze, Hotjar e Selenium, aliadas a métricas padronizadas, permitem melhorias contínuas e focadas em reduzir erros críticos e aumentar a confiança do usuário.